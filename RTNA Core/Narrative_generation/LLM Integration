# core/llm_integration.py

import openai
import requests
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

class LLMIntegration:
    """
    Handles integration with different Large Language Models (LLMs).
    """

    def __init__(self, provider="huggingface", model_name="gpt2", api_key=None):
        """Initializes the LLM integration."""
        self.provider = provider.lower()
        self.model_name = model_name
        self.api_key = api_key

        if self.provider == "openai":
            if not self.api_key:
                raise ValueError("API key is required for OpenAI integration.")
            openai.api_key = self.api_key
        elif self.provider == "aistudio":
            if not self.api_key:
                raise ValueError("API key is required for AI Studio integration.")
            self.aistudio_url = "https://aistudio.baidu.com/api/v1/...."  # Replace with actual API endpoint

        elif self.provider == "huggingface":
            self.generator = pipeline('text-generation', model=self.model_name)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")

    def generate_text(self, prompt: str, max_tokens: int = 50) -> str:
        """Generates text using the selected LLM."""
        if self.provider == "openai":
            response = openai.Completion.create(
                engine=self.model_name, 
                prompt=prompt,
                max_tokens=max_tokens
            )
            return response.choices[0].text.strip()

        elif self.provider == "aistudio":
            headers = {"Authorization": f"token {self.api_key}"}
            data = {"prompt": prompt, "max_tokens": max_tokens}
            response = requests.post(self.aistudio_url, headers=headers, json=data)
            if response.status_code == 200:
                return response.json()["text"].strip()
            else:
                raise Exception(f"AI Studio API request failed: {response.status_code}")

        elif self.provider == "huggingface":
            output = self.generator(prompt, max_length=max_tokens, num_return_sequences=1)
            return output[0]['generated_text'].strip()

        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
